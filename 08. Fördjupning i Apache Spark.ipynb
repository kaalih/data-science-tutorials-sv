{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fördjupning i Apache Spark\n",
    "Nu när vi har gått igenom grunderna för Spark så kan vi titta på hur vi kan göra mer sofistikerad bearbetning av data och applicera analytiska funktioner. Även om Spark data frames är ett relativt nytt koncept så är funktionaliteten redan kraftfull och förbättras för varje ny release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins \n",
    "Joins i Spark fungerar på samma sätt som i SQL och Pandas och görs genom funktionen `.join()` som finns på alla objekt av typen data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bet = hc.table('analytics_prod_11.bet')\n",
    "draw = hc.table('analytics_prod_11.draw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join_cond = (bet['draw_number'] == draw['draw_number']) & (bet['gem_product_id'] == draw['gem_product_id'])\n",
    "\n",
    "joined = bet.join(draw, join_cond, 'leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I det här fallet joinar vi på en nyckel som heter likadant i båda tabellerna vilket innebär att vi får dubletter av fältet `draw_number`. Om vi försöker selektera detta fältet får vi ett felmeddelande enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined.select('draw_number').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att hantera detta kan vi prefixa kolumnen med dess ursprungstabell enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined.select('bet.draw_number').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftast är det dock enklast att droppa eller döpa om dublettkolumnerna för att slippa få framtida fel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = (joined.drop(joined['draw.draw_number'])\n",
    "          .drop(joined['draw.dt'])\n",
    "          .drop(joined['draw.gem_product_id'])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cachning av data\n",
    "Spark är ett in-memory-baserat ramverk som håller data i minnet under tiden en operation exekveras. Problemet är att minnet släpps när operationen är avslutad vilket gör att man under en explorativ analys exempelvis kör exakt samma jobb flera gånger vilket adderar massor av tid. Ett sätt att komma runt detta är att cacha det datat man vill arbeta med tills man har jobbat sig igenom sin analys och i skede två släppa cachen och köra på full volym.\n",
    "\n",
    "Nedan är ett sätt att ta ett sample av datat och cacha detta för att få upp hastigheten i efterföljande operationer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached = (joined.filter(\"dt >= '2014-01-01'\")\n",
    "          .sample(False, 0.0001)\n",
    "          .cache()\n",
    "          )\n",
    "\n",
    "cached.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolumnoperationer\n",
    "För att enklare kunna komma åt alla kolumnfunktioner kan vi deklarera en ny variabel `x` till kolumnen `cached['customer_id]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = cached['customer_id']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolumner i Spark har en hel del inbyggda funktioner precis som Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.alias('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.isNull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa kan användas i selekteringar enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cached.select(cached['customer_id'].alias('kund'), cached['draw_number'].isNotNull(), 'register_ts')\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "För att komma åt flera functioner kan modulen `functions` importeras. Jag brukar importera med alias `f` för att underlätta användningen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.expr(\"wager_sg_1_sek + wager_sg_2_sek\").alias('wager_sek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.select(f.to_date('register_ts'),\n",
    "              f.current_date(),\n",
    "              'wager_sg_1_sek',\n",
    "              f.lit('konstant'),\n",
    "              f.year('bet.dt'),\n",
    "              f.when(cached['wager_sg_1_sek'] > 50, 'BIG BET').otherwise('small bet').alias('kind_of_bet'),\n",
    "              f.expr(\"wager_sg_1_sek + wager_sg_2_sek\").alias('wager_sek')\n",
    "              ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi vill spara resultatet av operationerna ovan behöver vi deklarera det som en ny data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = cached.select(f.to_date('register_ts').alias('trans_date'),\n",
    "              f.current_date().alias('curr_date'),\n",
    "              'wager_sg_1_sek',\n",
    "              f.lit('konstant'),\n",
    "              f.year('bet.dt').alias('year'),\n",
    "              f.when(cached['wager_sg_1_sek'] > 50, 'BIG BET').otherwise('small bet').alias('kind_of_bet'),\n",
    "              f.expr(\"wager_sg_1_sek + wager_sg_2_sek\").alias('wager_sek')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi endast vill addera en kolumn till en befintliga data fram kan vi göra det med funktionen `.withColumn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.withColumn('date_diff', f.datediff(result['curr_date'], result['trans_date'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men även här måste vi spara vårt resultat i en ny variabel för att *spara* förändringen. Om vi kör `result.show(5)` så ser vi inte kolumnen `date_diff` ovan. Detta är ungefär samma beteende som Pandas har med undantaget att man använder inPlace. Spark är helt **immutable** vilket innebär att ett dataset aldrig kan ändras utan bara transformeras till nya dataset. Det är standard i distribuerade system och ett önskvärt beteende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hantering av nullvärden i Spark\n",
    "Spark har grundläggande hantering av nullvärden vilka är grupperade under dataframefunktionen `.na`. För att droppa rader med nullvärden kan man exempelvis använda funktionen `.na.drop()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan introducera några nullvärden för att se hur Spark kan hantera det."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "where_clause_1 = f.when(result['kind_of_bet'] == 'small bet', None).otherwise(result['kind_of_bet'])\n",
    "where_clause_2 = f.when(result['wager_sg_1_sek'] == 25.00, None).otherwise(result['wager_sg_1_sek'])\n",
    "\n",
    "null_result = (result.withColumn('null_col', where_clause_1)\n",
    "               .withColumn('null_col_2', where_clause_2)\n",
    "               )\n",
    "\n",
    "null_result.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_result.na.drop().show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovan ser vi att Spark hittar de nullvärden vi introducerade och droppar de raderna. För att istället ersätta värden kan vi använda funktionen `.na.fill()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_result.na.fill(50).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi studerar resultatet ovan ser vi att Spark ignorerar kolumner där datatyperna inte matchar. Det innebär att vårt numeriska värde 50 enbart appliceras på `null_col_2` och inte på den icke-numeriska kolumnen `null_col`.\n",
    "\n",
    "Här får vi antingen applicera funktionen två gånger eller skapa en `dict` med en `kolumn : värde`-mappning enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_result.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'null_col' : '50', 'null_col_2' : 80}\n",
    "\n",
    "null_result.na.fill(d).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregeringar och group by operationer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = cached.groupby('gem_product_id')\n",
    "type(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla direkta funktioner på vårt GroupedData objekt applicerar på alla numeriska kolumner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb.sum().limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativt kan vi vara mer specifika i vad vi vill göra genom att skicka in valfria aggregeringsfunktioner så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.groupby('gem_product_id').agg(f.sum('wager_sg_1_sek'), f.avg('wager_sg_1_sek'), f.countDistinct('customer_id')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan också skicka in en dict som mappar kolumn till transformation så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = {'wager_sg_1_sek' : 'sum', 'wager_sg_2_sek' : 'mean'}\n",
    "\n",
    "gb.agg(d).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivotteringar i Spark\n",
    "Spark har grundläggande funktionalitet för att skapa pivottabeller av typen vi gått igenom i Pandas. Om vi exempelvis vill summera försäljning per år kan vi göra det så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = cached.select(f.month('dt').alias('month'), f.year('dt').alias('year'), '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot = (df.groupby('gem_product_id')\n",
    "         .pivot('year')\n",
    "         .sum('wager_sg_1_sek')\n",
    "         .toPandas()\n",
    "        )\n",
    "\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Vi kan även göra flera summeringar med funktionen `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot = (df.groupby('gem_product_id')\n",
    "         .pivot('year')\n",
    "         .agg(f.sum('wager_sg_1_sek'), f.countDistinct('customer_id'))\n",
    "         .toPandas()\n",
    "        )\n",
    "\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fönsterfunktioner\n",
    "Spark har ett väl fungerande stöd för fönsterfunktioner av olika slag. Grundfunktionen är att man specificerar ett fönster och anger `.partitionBy()` och `.orderBy()`. Därefter har man tillgång till en mängd fönsterfunktioner i biblioteket `pyspark.sql.functions` som vi tidigare har importerat som `f`.\n",
    "\n",
    "Därefter kan man använda funktioner som arbetar över det definierade fönstret enligt `function().over(window)` som nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(cached['customer_id']).orderBy(cached['dt'].asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cached.select('customer_id', f.row_number().over(w).alias('row_num')).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Om vi vill räkna på spelfrekvens och varians utifrån dagar mellan speltillfällen kan vi relativt enkelt göra det genom att först räkna ut dagar mellan spel så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player_bets = (cached.select('customer_id', 'dt', f.lag('dt', 1, None).over(w).alias('last_dt')) \n",
    "               .withColumn('diff_dt', f.datediff('dt', 'last_dt'))\n",
    "              )\n",
    "\n",
    "player_bets.filter(\"diff_dt > 0\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "När vi har de nödvändiga kolumnerna redo kan vi enkelt gruppera på `customer_id` och aggregera med avg och stddev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player_freq = (player_bets.groupBy('customer_id')\n",
    "               .agg(f.avg('diff_dt').alias('avg_days_between_bets'),\n",
    "                    f.stddev('diff_dt').alias('stddev_days_between_bets'))\n",
    "              )\n",
    "\n",
    "player_freq.filter(\"avg_days_between_bets > 0 and stddev_days_between_bets != 'NaN'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidsinterval i Spark\n",
    "I Spark kan man använda tidsuttryck i form av exempelvis `interval 30 days` för att räkna fram olika tidsintervall. Man använder dessa uttryck med funktionen `f.expr()` så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.expr('interval 30 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Därefter kan man använda uttrycken i selectuttryck så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player_bets.select(player_bets['dt'] + f.expr('interval 90 days'), 'dt').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eller som filteruttryck så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bets_placed_within_2_weeks = player_bets.filter(\"last_dt + interval 2 weeks > dt\")\n",
    "bets_placed_within_2_weeks.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bearbetning av nästlade loggar i Spark\n",
    "En stor del av det data vi har i vår lake består av nästlade strukturer. För att kunna få ut rätt information är det centralt att kunna *platta ut* datat på ett korrekt sätt. Som tur är har Spark fantastiskt bra stöd för den typen av operationer vilket vi ska gå igenom här.\n",
    "\n",
    "Vi börjar med att läsa in en loggfil innehållande speltransaktioner för `2016-05-11` på samma sätt som vi har gjort tidigare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/svsdata/argon_prod/ItsRegWager/dt=2016-05-11'\n",
    "itsregwager = hc.jsonRDD(sc.sequenceFile(path).values(), samplingRatio=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi kör `.printSchema()` ser vi hela strukturen och hur den är nästlad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itsregwager.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan börja med att ta ut `Wager` som är det fältet som innehåller speltransaktionerna och cacha det för att snabba upp efterföljande bearbetning. Om vi nu visar några rader data så ser vi att vi enbart har en kolumn på översta nivån."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wager = itsregwager.select('Wager').cache()\n",
    "wager.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att selektera underliggande fält kan vi använda syntaxen `fält1.fält2` för att ta oss ner i strukturen. Vi får bra resultat för de direkt underliggande fälten men för `Bets` som är en lista ser det inte lika relevant ut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wager.select('Wager.CustomerId', 'Wager.PartnerId', 'Wager.Bets').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan enkelt gå ett steg djupare i hierarkin så här vilket ger något bättre överblick men vi ser tydligt här att flera rader har 2 stycken poster i fälten `ProductId` och `Amount1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wager.select('Wager.CustomerId', 'Wager.PartnerId', 'Wager.Bets.ProductId', 'Wager.Bets.Amount1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wager.select('Wager.CustomerId', 'Wager.PartnerId', 'Wager.Bets.ProductId', 'Wager.Bets.Amount1').limit(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det vi kan göra här är att pivotera fältet `Bets` så att vi istället för en lista får en ny rad för varje element. För att åstadkomma det kan vi använda oss av funktionen `f.explode()`. I resultatet av den operationen ser vi att vi istället för en rad för första kunden nu får två rader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bets = wager.select('Wager.CustomerId', 'Wager.PartnerId', f.explode('Wager.Bets').alias('B'))\n",
    "bets.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan också enkelt se effekten genom att räkna antalet rader före och efter pivotteringen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wager.count()\n",
    "print bets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi nu väljer ut samma kolumner som vi gjorde ovan ser vi att vi inte längre har flera spel på samma rad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bets.select('CustomerId', 'PartnerId', 'B.ProductId', 'B.Amount1', 'B.Amount2').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att snygga upp det lite kan vi exempelvis göra enligt nedan för att bland annat få beloppen i kronor istället för ören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = bets.select('CustomerId', \n",
    "            'PartnerId', \n",
    "            f.split(bets['B.ProductId'], '-')[0].alias('ProductId'),\n",
    "            f.split(bets['B.ProductId'], '-')[1].alias('ProductName'), \n",
    "            f.expr('B.Amount1 / 100').alias('Amount1_Sek'),\n",
    "            f.expr('B.Amount2 / 100').alias('Amount2_Sek'),\n",
    "           \n",
    "           )\n",
    "                    \n",
    "\n",
    "\n",
    "#final.filter(\"n_boards > 1\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    "Om vi har behov av att göra något särskilt kan vi väldigt smidigt definiera egna funktioner och registrera dessa som udf:er i Spark. Exemplet nedan använde jag för att räkna fram dragningssekvenser för flerveckorsspel. \n",
    "\n",
    "Funktionen tar en input i form av första dragningsnumret och en integer för längden på sekvensen och returnerar en lista med integers. Vi kan testa funktionen lokalt i Python för att se att den returnerar rätt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequence(draw, seqlength):\n",
    "    draw = int(draw)\n",
    "    seq = [draw]\n",
    "    \n",
    "    while len(seq) < seqlength:\n",
    "        next_draw = seq[-1] + 1\n",
    "        seq.append(next_draw)\n",
    "    \n",
    "    return seq\n",
    "\n",
    "sequence(1530, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att använda denna i Spark kan vi göra det så här. Funktionen  `f.udf()` tar en funktion och behöver definiera en `returnType` som i det här fallet är en array av integers eller `ArrayType(IntegerType())` i Sparks värld. \n",
    "\n",
    "Vi behöver importera dessa typer innan vi kan använda dom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "seq = f.udf(sequence, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu när vi har registrerat funktionen kan vi lätt använda den i exempelvis en select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = final.select('ProductId', \n",
    "             seq('ProductId', f.lit(3)).alias('tmp')\n",
    "            )\n",
    "\n",
    "t.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.select('ProductId', f.explode('tmp')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
