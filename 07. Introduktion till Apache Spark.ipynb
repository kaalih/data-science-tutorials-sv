{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduktion till Apache Spark\n",
    "Apache Spark är ett distribuerat ramverk för att hantera stora datamängder som är extremt hett just nu. Till skillnad mot MapReduce så använder sig Spark av minnesstrukturer vilket snabbar upp processningen avsevärt, särskilt på iterativa flöden som machine learning. \n",
    "\n",
    "Spark innehåller komponenter för allt från strömprocessning, sql och dataframes, machine learning och grafhantering. Vi kommer inte hinna gå igenom allt men ska försöka täcka in de områden som jag bedömer är de vi kommer att arbeta mest med vilket är dataframes och SQL.\n",
    "\n",
    "Läs gärna mer på Sparks webbsida <a href=\"http://spark.apache.org/\">http://spark.apache.org/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Läsa och skriva data\n",
    "I Jupyter har vi tillgång till i princip all data vi har på Svenska Spel. Vi kan använda oss av både Spark och Pandas för att läsa och skriva data. Med tanke på de generella datavolymer vi kommer att behöva hantera kommer Spark att vara det ramverk som används oftast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läsa data från Hive/Hadoop\n",
    "Spark har inbyggt stöd för att läsa från och skriva till Hive. För att arbeta med data lagrad i Hive behöver man starta upp ett `HiveContext()` enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att lista tillgängliga tabeller kan man använda funktionen `.tableNames()` vilket returnerar en lista på tabeller i ett givet schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.tableNames('analytics_prod_11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att arbeta med data från en tabell kan man antingen skapa en data frame direkt från en tabell eller så kan vi använda Spark för att ställa frågor via SQL. För att skapa en data frame direkt används funktionen `.table()` genom att ange `schema.tabell` så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oms = hc.table('analytics_prod_11.oms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oms.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "För att ställa en sqlfråga mot Hive använder vi funktionen `.sql()` som tar en fråga som input och levererar resultatet i form av en data frame. Ofta är det enklast att spara frågan i en separat variabel som sedan skickas in i funktionen enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "select gem_product_id, sum(wager_sg_1_sek) as sg1, sum(wager_sg_2_sek) as sg2\n",
    "from analytics_prod_11.bet\n",
    "where dt >= '2016-01-01'\n",
    "group by gem_product_id\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = hc.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skriva data till Hive\n",
    "Om vi vill skriva exempelvis resultat ovan till en tabell i Hive kan vi göra det via funktionen `.write.saveAsTable()` enligt nedan. Funktionen tar argumenten `schema.tabell`, `format` och `mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.write.saveAsTable('user_dabc.results', 'orc', 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan validera att vi lyckats genom att se att tabellen finns i schemat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.tableNames('user_dabc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läsa filer från Oracle eller annan jdbc-källa\n",
    "Spark kan läsa data via jdbc genom att ange den tabell man vill komma åt enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_customer = hc.read.jdbc(url='jdbc:oracle:thin:dabc/Sommar2014@hexa-scan.vby.svenskaspel.se:1521/dwdb', \n",
    "                          table='dmuser.d_customer', \n",
    "                          properties={'driver':'oracle.jdbc.driver.OracleDriver'}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_customer.select('CUSTOMER_KEY', 'GENDER_NAME', 'EMPLOYEE').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det går också att istället för en tabell ange en fråga i form av en subquery, med andra ord en selectsats med paranteser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subquery = \"\"\"\n",
    "\n",
    "(select year_no, f.date_key as transaction_date, customer_number as customer_id, sum(amount_rake) as rake, 'poker' as product_id \n",
    "from dmuser.v_cube_d_poker_sales f\n",
    "inner join dmuser.d_date d on f.date_key = d.date_key\n",
    "inner join dmuser.d_customer c on f.customer_key = c.customer_key\n",
    "where year_no >= 2014\n",
    "group by year_no, f.date_key, customer_number)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poker = hc.read.jdbc(url='jdbc:oracle:thin:dabc/Sommar2014@hexa-scan.vby.svenskaspel.se:1521/dwdb', \n",
    "                     table=subquery, \n",
    "                     properties={'driver':'oracle.jdbc.driver.OracleDriver'}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poker.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poker.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om man ska köra många frågor är det ju lättare att lägga `url` och `properties` i variabler som går att återanvända."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'jdbc:oracle:thin:dabc/Sommar2014@hexa-scan.vby.svenskaspel.se:1521/dwdb'\n",
    "properties = {'driver':'oracle.jdbc.driver.OracleDriver'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = hc.read.jdbc(url=url, \n",
    "                     table='dmuser.d_product', \n",
    "                     properties=properties\n",
    "                    )\n",
    "\n",
    "tmp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp.write.saveAsTable('user_dabc.tmp_product', 'orc', 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Skriva till Oracle\n",
    "Spark har från och med version 1.6.2 möjlighet att skriva data till Oracle efter att tidigare ha haft problem med datatyper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poker.write.jdbc(url, 'sasuser.dabc_jdbc_test', 'overwrite', properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skriva resultatet av hivefrågor till fil (fungerar enbart via ssh till hdg01 just nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hive -e 'select * from analytics_prod_11.oms' > /home/dabc/temp.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Läsa filer i vår data lake\n",
    "På Svenska Spel har vi en data lake-strategi vilket innebär att vi lagrar data i dess ursprungsform vilket i många fall är jsonstrukturer. En av de stora fördelarna med Spark data frames är att det finns mycket bra stöd för att tolka och bearbeta data som inte har en tabulär struktur.\n",
    "\n",
    "Då hadoop lagrar vårt data i formatet `sequencefile` kan vi göra enligt nedan för att hitta och läsa datat. Vi kan börja med att använda hdfs-klienten i Jupyter för att lista hadoops filstruktur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vår data lake ligger under katalogen svsdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /svsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här ser vi flera foldrar och den vi är intresserad av är argon_prod. Genom att ange `| head` kan vi begränsa antalet poster som listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /svsdata/argon_prod | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi tittar under en speciell katalog ser vi att datat ligger partitionerat per datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /svsdata/argon_prod/ItsRegWager | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att läsa dessa tabeller kan vi sätta upp en läsare mot våra sekvensfiler så här. Notera de olika sätt vi kan strypa hur mycket data vi läser upp med hjälp av olika wildcards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# För att läsa ett helt år\n",
    "# sc.sequenceFile('/svsdata/argon_prod/ItsRegWager/dt=2006*')\n",
    "\n",
    "# För att läsa ett flera år\n",
    "# sc.sequenceFile('/svsdata/argon_prod/ItsRegWager/dt={2015,2016}*')\n",
    "\n",
    "# För att läsa en månad\n",
    "# sc.sequenceFile('/svsdata/argon_prod/ItsRegWager/dt=2016-04*')\n",
    "\n",
    "# Eller för att läsa ett specifik datum\n",
    "seq = sc.sequenceFile('/svsdata/argon_prod/ItsRegWager/dt=2006-03-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I det här läget är datat lagrat som ett dataset av typen RDD (resilient distributed dataset) vilket är Sparks primära datastruktur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan titta på första raden för att se hur det ser ut med funktionen `.first()`. Som vi ser är varje rad en key-value-struktur där datat är representerat som json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Då vi bara är intresserade av jsonstrukturen kan vi enkelt komma åt den genom `.values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_v = seq.values()\n",
    "seq_v.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_v.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu när vi har vårt data i en RDD som består av json så kan vi enkelt skapa upp en data frame med funktionen `.jsonRDD()`. Den här funktionen kommer att scanna igenom datat och derivera fram ett schema som vi sedan kan använda oss av för att bearbeta datat. \n",
    "\n",
    "Notera att det är viktigt att ange parametern `samplingRatio` för att berätta hur stor del av datat Spark ska använda för att tolka datat. I och med att vi har variabla strukturer behöver Spark i en del fall läsa hela datasetet medans det i andra fall räcker med en liten andel. Här får man ibland prova sig fram.\n",
    "\n",
    "I och med att vi i det här fallet har 154052 rader i datat kan vi nöja oss med en mindre andel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bets = hc.jsonRDD(seq_v, samplingRatio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "När Spark har tolkat datat färdigt kan vi se vad vi får tillbaka för schema med funktionen `.printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det går som vanligt att komprimera läsning och tolkning till en instruktion, exempelvis så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet = hc.jsonRDD(sc.sequenceFile('/svsdata/argon_prod/ItsRegWager/dt=2016-03*').values(), samplingRatio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan visa de översta 5 raderna som vanligt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark data frames\n",
    "Spark har liksom Pandas ett koncept för data frames. Spark har försökt att i möjligaste mån ligga så nära Pandas som möjligt vilket gör att det är relativt enkelt att komma igång med Spark om man tidigare har arbetat med Pandas. Vi börjar med att läsa in lite data som vi kan arbeta med."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet = hc.table('analytics_prod_11.bet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan även returnera alla kolumner i form av en lista via attributet `.columns`. Detta kan vara användbart om vi vill arbeta med metadata programmatiskt för att exempelvis loopa och applicera funktioner på många kolumner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = bet.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    print 'Do something with ' + col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan enkelt räkna antalet rader med funktionen `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att droppa en rad använder vi funktionen `.drop()`. Funktionen returnerar en ny data frame så vi måste deklarera en ny variabel för att ta emot detta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = bet.drop('addon')\n",
    "dropped.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark använder sig av något som kallas för `lazy evaluation` vilket innebär att inga operationer exekveras förrän man begär en outputoperation. Detta skiljer sig från pandas där man har resultatset i minnet.\n",
    "\n",
    "### Selekteringar i Spark\n",
    "Om vi exempelvis vill välja ut några kolumner i Spark kan vi köra funktionen `.select()` och skicka in de kolumnnamn vi vill ha. Som ni märker sker ingen exekvering i det här läget utan Spark bygger endast upp en del i ett exekveringsträd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection = bet.select('wager_serial', 'customer_id', 'wager_sg_1_sek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan se att Spark ändå har gjort en transformation genom `.printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi nu vill se resultatet så kommer Spark att köra allt ovan och returnera ett resultatset enligt begärt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selection.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtreringar och reducering av datamängder\n",
    "För att filtrera på data kan vi använda funktionen `.filter()` och lägga in det villkor vi vill köra enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = bet.filter(\"dt >= '2016-01-01'\")\n",
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan också applicera funktionerna `.limit()` eller `.sample()` för att reducera mängden data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered.limit(100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered.sample(False, 0.01).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi har en mindre mängd data så kan vi enkelt konvertera en data frame i Spark till en data frame i Pandas. Det är dock viktigt att tänka på vilka volymer data man arbetar med i det här fallet. Om vi exempelvis skulle försöka lyfta över hela tabellen bet som är på 3,5 miljarder rader kommer vi att köra slut på minne och krascha Sparkjobbet. \n",
    "\n",
    "En vettig volym ligger på 2-3 miljoner rader. Fördelen med att lyfta till Pandas är att vi får tillgång till ett rikare utbud av funktioner och att Pandas visar data på ett sätt som passar bättre i en notebook.\n",
    "\n",
    "om vi som ovan tar ett sample på 1% av datavolymen ser vi att vi landar på en hanterbar volym data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = filtered.sample(False, 0.01)\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att lyfta detta till Pandas använder vi funktionen `.toPandas()`. Det tar en liten stund att flytta datat från klustret till jupyterservern men när det väl är flyttat går det mycket snabbare att arbeta med. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_df = sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi tittar på datatyperna ser vi att vi har en data frame på Spark och en på Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(sample)\n",
    "print type(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan använda oss av funktionen `.info()` på pandas_df för att se hur mycket minne den tar. I det här fallet 123mb vilket inte är särskilt mycket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "Sortering av en data frame kan göras med funktionen `.sort()` vilken tar valfritt antal kolumner som input. Kolumner i Spark deklareras på samma sätt som Pandas enligt `df['kolumn']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bet = bet.filter(\"dt > '2015-12-31'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet['dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "För att ange sortering på kolumnen kan `.asc()` eller `.desc()` användas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet['dt'].desc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "För att sortera tabellen `bet` fallande efter datum skriver man såhär."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet.sort(bet['dt'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan sortera på fler begrepp så här."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet.sort(bet['dt'].desc(), bet['customer_id'].asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL med Spark\n",
    "Ovan har vi använt oss av Sparks objektorienterade API mot våra data frames. Vi kan också enkelt registrera en data frame som en temporär tabell som vi kan skriva SQL mot. För detta anävnder vi oss av funktionen `.registerTempTable()` enligt nedan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample.registerTempTable('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "När vi har gjort detta kan vi ställa SQL-frågor mot tabellen som om den vore en fysisk tabell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc.sql(\"\"\"\n",
    "\n",
    "select dt, sum(wager_sg_1_sek) as sales from tmp\n",
    "group by dt\n",
    "order by dt asc\n",
    "limit 10\n",
    "\n",
    "\"\"\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
